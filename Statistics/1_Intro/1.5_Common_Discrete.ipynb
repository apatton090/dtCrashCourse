{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Statistics\n",
    "## Chapter 1.5 - Common Discrete Random Variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Random Variable\n",
    "The simplest experiment we can have is one with 2 possible outcomes. Typically, the outcomes are referred to as \"success\", the one (most) related to the question of interest, and \"failure\". Mathematically, the outcomes are encoded as 1 or 0 respectively. This type of random variable is called a Bernoulli random variable or a Bernoulli trail (name after Jacob Bernoulli so it is capcitalized). The remain discrete random variables introduced in this chapter will be derived from the Bernoulli.\n",
    "\n",
    "Suppose probability of success, $P(X=1)$, is some constant $p$. Then $P(X=0)$ must be $1-p$ because it is the complement of $X=1$. We can combine them together to form the following pmf:\n",
    "- (1) $p(x) = \\begin{cases} p, \\quad \\text{if }x=1 \\\\ 1-p, \\quad \\text{if }x=0 \\end{cases}$  \n",
    "\n",
    "Using the pmf, we can derive the expected value and variance  \n",
    "### Expected Value and Variance\n",
    "- (2) $E(X) = p$\n",
    "    - $E(X) = \\sum_x xp(x) = 0(1-p) + 1*p = p$  \n",
    "- (3) $Var(X) = p(1-p)$\n",
    "    - $Var(X) =\\sum_x (x-\\mu)^2p(x) = \\sum_x (x-p)^2p(x) = (1-p)^2p + (0-p)^2(1-p)$\n",
    "    - $= p - 2p^2 +p^3 + p^2 - p^3 = p - p^2 = p(1-p)$\n",
    "\n",
    "### Notation\n",
    "Suppose a random variable $X$ is distributed as a Bernoulli with parameter $p$. The short hand notation is:\n",
    "- $X \\sim Bern(p)$\n",
    "    - $\\sim$ means distributed as\n",
    "\n",
    "Suppose the random variable $X$ above is a coin flipping experiment, with $X=1$ if the coin is heads and $X=0$ otherwise (sometimes shorten to o.w.). If we repeat the coin flipping experiment $n$ times, then we would generate a sequence of random variables, $X_1, X_2, X_3, ..., X_n$. These random variables are said to be independent because the outcome of one experiment will not effect the outcome of another, and indentically distributed because we are using the same coin. We are assuming that the coin behavior does not change between experiments, therefore having the same probability distribution (same type of random variables with same parameters). Independent and indentically distributed is typically shorten to iid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomial\n",
    "Binomial random variable models the number of successes in $n$ iid Bernoulli trials. Since success is encoded as a 1 for a Bernoulli, Binomial can be seen as the sum of $n$ iid Bernoulli trials.\n",
    "\n",
    "Now lets derive the binomial pmf.\n",
    "- Given: $n$ number of Bernoulli trails, $x$ number of successes\n",
    "- We know that the probability of success for a Bernoulli is $p$. So the probability of x number of success is $p^x$\n",
    "- We know that the probability of failure for a Bernoulli is $1-p$ and the number of failure is $n-x$. So the probability of $n-1$ number of failure is $(1-p)^{n-x}$\n",
    "- There are many ways to have $x$ number of successes in $n$ trails. We can calculate the number of ways by using combinations, $\\binom nx$\n",
    "\n",
    "By combining everything above, we have the following pmf:\n",
    "- (4) $p(x) = \\binom n x p^x (1-p)^{n-x}$\n",
    "\n",
    "The notation of a random variable $X$ being distributed as a binomial is:\n",
    "- $X \\sim B(n, p)$\n",
    "\n",
    "### Expected Value and Variance\n",
    "We could derive both of these from the pmf. However, it is significantly easier to derive them using a sequence of Bernoulli random variables, $Y_1, ..., Y_n$, instead.\n",
    "- (5) $E(X) = np$\n",
    "    - $E(X) = E(Y_1 + Y_2 + ... + Y_n) = E(Y_1) + ... + E(Y_n) = np$\n",
    "- (6) $Var(X) = np(1-p)$\n",
    "    - $Var(X) = Var(Y_1 + Y_2 + ... + Y_n) = Var(Y_1) + ... + Var(Y_n) = np(1-p)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson\n",
    "\n",
    "Poisson (named after Sim√©on Denis Poisson) is used to model the number of times something occurs over a fixed length of time, area, or volume.\n",
    "\n",
    "Imagine we are tring to model the number of car accidents over a year. We can try to model this as a binomial. We can split the year into $n$ intervals, and each interval will be a Bernoulli trail on whether a car accident occurred. We cannot have multiple car accident during the same time intervals because then it wont be a Bernoulli. So we have to have $n=\\infty$ for the probability of multiple car accident during the same intervals be $0$. Now we have the following equation:\n",
    "- $\\lim_{n \\to \\infty} \\binom n x p^x (1-p)^{n-x}$  \n",
    "\n",
    "Now we have another problem, $p$ must equal $0$ or else $n*p$ (expected value of binomial) will blow up to $\\infty$. Setting $p=0$ will cause our pmf to collapse to 0. Lets set $\\lambda=np$. Then we can subsitute $p$ with $\\frac{\\lambda}{n}$.\n",
    "- $\\lim_{n \\to \\infty} \\binom n x \\Big( \\frac{\\lambda}{n}\\Big) ^x (1- ( \\frac{\\lambda}{n}) ^{n-x}$  \n",
    "\n",
    "We can simplify this to get our new pmf, the pmf of a Poisson:\n",
    "- (7) $p(x) = \\lim_{n \\to \\infty} \\binom n x \\frac{\\lambda}{n}^x (1-\\frac{\\lambda}{n})^{n-x} = \\frac{\\lambda^x e^\\lambda}{x!}$\n",
    "    - $\\lim_{n \\to \\infty} \\binom n x \\Big( \\frac{\\lambda}{n}\\Big) ^x (1- \\Big( \\frac{\\lambda}{n})\\Big) ^{n-x} = \\lim_{n \\to \\infty} \\frac{n!}{(n-x)!x!} \\Big( \\frac{\\lambda}{n}\\Big) ^x (1-  \\frac{\\lambda}{n}) ^{n-x} = \\lim_{n \\to \\infty} \\frac{n!\\lambda^x}{(n-x)!x!n^x} \\lim_{n \\to \\infty} (1-  \\frac{\\lambda}{n}) ^{n-x}$\n",
    "        - $\\lim_{n \\to \\infty} (1-  \\frac{\\lambda}{n}) ^{n-x} = \\lim_{n \\to \\infty} (1-  \\frac{\\lambda}{n}) ^{n} = e^{-\\lambda}$\n",
    "    - $= \\lim_{n \\to \\infty} \\frac{n!\\lambda^x}{(n-x)!x!n^x} e^{-\\lambda} = \\frac{\\lambda^x e^{-\\lambda}}{x!} \\lim_{n \\to \\infty} \\frac{n!}{(n-x)!n^x} = \\frac{\\lambda^x e^{-\\lambda}}{x!} \\lim_{n \\to \\infty} \\frac{n!}{(n-x)!} \\lim_{n \\to \\infty} \\frac{1}{n^x}$\n",
    "        - $\\lim_{n \\to \\infty} \\frac{n!}{(n-x)!} = \\lim_{n \\to \\infty} n^x$\n",
    "    - $= \\frac{\\lambda^x e^{-\\lambda}}{x!} \\lim_{n \\to \\infty} n^x \\lim_{n \\to \\infty} \\frac{1}{n^x} = \\frac{\\lambda^x e^{-\\lambda}}{x!} \\lim_{n \\to \\infty} \\frac{n^x}{n^x} = \\frac{\\lambda^x e^{-\\lambda}}{x!}$\n",
    "\n",
    "## Expected Value and Variance\n",
    "- (8) $E(X) = \\lambda$\n",
    "- (9) $Var(X) = \\lambda$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others\n",
    "There are many other discrete random variables. Here is a list of some other common ones and their purpose.\n",
    "- Geometric\n",
    "    - Models the number of Bernoulli trials needed to reach the first success\n",
    "- Negative Binomial\n",
    "    - Models the number of Bernoulli trials needed to reach r number of success\n",
    "- Hypergeometric\n",
    "    - Models the number of successful draws from a population of size $N$ with $K$ number of objects of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equations\n",
    "Bernoulli  \n",
    "1) $p(x) = \\begin{cases} p, \\quad \\text{if }x=1 \\\\ 1-p, \\quad \\text{if }x=0 \\end{cases}$  \n",
    "2) $E(X) = p$  \n",
    "3) $Var(X) = p(1-p)$\n",
    "\n",
    "Binomial\n",
    "4) $p(x) = \\binom n x p^x (1-p)^{n-x}$  \n",
    "5) $E(X) = np$  \n",
    "6) $Var(X) = np(1-p)$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}